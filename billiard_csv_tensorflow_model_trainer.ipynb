{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esemlak/esemlak/blob/main/billiard_csv_tensorflow_model_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toCaSzfhLJTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4002eb0d-ca2f-4577-b810-9ed8fdfb2425"
      },
      "source": [
        "!unzip -o /content/colab_train_big.zip\n",
        "!pip3 install keras-tuner"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/colab_train_big.zip\n",
            "replace colab_train_big.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.6)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.8.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.18.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.48.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 113, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4849, in parseImpl\n",
            "    loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1716, in _parseNoCache\n",
            "    tokens = fn(instring, tokensStart, retTokens)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1316, in wrapper\n",
            "    ret = func(*args[limit[0]:])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 81, in <lambda>\n",
            "    lambda s, l, t: Marker(s[t._original_start : t._original_end])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/markers.py\", line 307, in __init__\n",
            "    self._markers = _coerce_parse_result(MARKER.parseString(marker))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4052, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1716, in _parseNoCache\n",
            "    tokens = fn(instring, tokensStart, retTokens)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1316, in wrapper\n",
            "    ret = func(*args[limit[0]:])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/markers.py\", line 132, in <lambda>\n",
            "    MARKER_OP.setParseAction(lambda s, l, t: Op(t[0]))\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 213, in _main\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1366, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.7/logging/handlers.py\", line 71, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1127, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 130, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 616, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 566, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 104, in print_exception\n",
            "    type(value), value, tb, limit=limit).format(chain=chain):\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 508, in __init__\n",
            "    capture_locals=capture_locals)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 363, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 285, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSFP3qG8woJK"
      },
      "source": [
        "DATASET_PATH = \"/content/colab_train_big.csv\"\n",
        "LABEL_NAME = \"SCORE\"\n",
        "\n",
        "# \"r\" for regression and \"c\" for classification\n",
        "# for classification, the number of output nodes is automatically determined\n",
        "TASK = \"r\"\n",
        "\n",
        "# don't change the dummy batch size. There is a small bug in tensorflow.\n",
        "# to get around that bug, dummy batch size is used\n",
        "DUMMY_BATCH_SIZE = 5\n",
        "BATCH_SIZE = 100\n",
        "EPOCHS = 100\n",
        "TRAIN_FRAC = 0.8\n",
        "\n",
        "CHECKPOINT_DIR = \"/content/\""
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVR1nxboHX45"
      },
      "source": [],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY-_wPnaCZbp"
      },
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import kerastuner as kt\n",
        "\n",
        "import IPython"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgbYDOHe0pu3"
      },
      "source": [
        "# This function will give us a fresh dataset object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FocJZX8lCZbw"
      },
      "source": [
        "def get_dataset(batch_size = 5):\n",
        "    return tf.data.experimental.make_csv_dataset(\n",
        "        DATASET_PATH,\n",
        "        batch_size = batch_size,\n",
        "        label_name = LABEL_NAME,\n",
        "        num_epochs = 1,\n",
        "        field_delim= ';'\n",
        "    )"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwNYZ_TOIPRJ"
      },
      "source": [
        "# If classification, Find Number of labels (for output nodes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GAe3626H0f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432cf10a-f58e-4a4e-e213-0c4362127861"
      },
      "source": [
        "unique_labels = set()\n",
        "\n",
        "unique_labels.add(\"VX\")\n",
        "unique_labels.add(\"VY\")\n",
        "num_labels = len(unique_labels)\n",
        "\n",
        "OUTPUT_NODES = num_labels\n",
        "    \n",
        "print(unique_labels)\n",
        "\n",
        "print(\"output nodes: \", OUTPUT_NODES)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'VY', 'VX'}\n",
            "output nodes:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test**"
      ],
      "metadata": {
        "id": "wYLmx1SqXIto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = {}\n",
        "\n",
        "#dataset = tf.data.experimental.make_csv_dataset(\n",
        "#        DATASET_PATH,\n",
        "#        batch_size = DUMMY_BATCH_SIZE,\n",
        "#        label_name = LABEL_NAME,\n",
        "#        num_epochs = 1,\n",
        "#        header = True,\n",
        "#    )\n",
        "\n",
        "#iterator = dataset.as_numpy_iterator()\n",
        "#print(next(iterator))"
      ],
      "metadata": {
        "id": "yKHml9nEXMFw"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi8fUZj40yG1"
      },
      "source": [
        "# Creating the Model Inputs as a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x-yO_2xCZb2"
      },
      "source": [
        "model_inputs = {}\n",
        "\n",
        "for batch, _ in get_dataset(batch_size=DUMMY_BATCH_SIZE).take(1):\n",
        "    for col_name, col_values in batch.items():\n",
        "        if (col_name == 'X1' or col_name == 'X2' or col_name == 'X3' or col_name == 'X4' or col_name == 'Y1' or col_name == 'Y2' or col_name == 'Y3' or col_name == 'Y4'):\n",
        "          model_inputs[col_name] = tf.keras.Input(shape=(1,), name=col_name, dtype=col_values.dtype)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPcuRa0xCZb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646afcd9-aa31-4527-c02b-84c893a6a5bb"
      },
      "source": [
        "model_inputs"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X1': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'X1')>,\n",
              " 'Y1': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Y1')>,\n",
              " 'X2': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'X2')>,\n",
              " 'Y2': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Y2')>,\n",
              " 'X3': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'X3')>,\n",
              " 'Y3': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Y3')>,\n",
              " 'X4': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'X4')>,\n",
              " 'Y4': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Y4')>}"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFS-00sa6x52"
      },
      "source": [
        "## split these inputs into their own dictionaries based on the data type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN7O52LACZcE"
      },
      "source": [
        "integer_inputs = {}\n",
        "float_inputs = {}\n",
        "string_inputs = {}\n",
        "\n",
        "for col_name, col_input in model_inputs.items():\n",
        "    \n",
        "    if col_input.dtype == tf.int32:\n",
        "        integer_inputs[col_name] = col_input\n",
        "    \n",
        "    elif col_input.dtype == tf.float32:\n",
        "        float_inputs[col_name] = col_input\n",
        "    \n",
        "    elif col_input.dtype == tf.string:\n",
        "        string_inputs[col_name] = col_input"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5jvxst6CZcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556034f1-0e5c-406d-f2f2-cba629ba6940"
      },
      "source": [
        "integer_inputs"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjSSO5srCZcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8956485-cd39-48bd-e090-a81132b60bb2"
      },
      "source": [
        "float_inputs"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X1': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'X1')>,\n",
              " 'Y1': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Y1')>,\n",
              " 'X2': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'X2')>,\n",
              " 'Y2': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Y2')>,\n",
              " 'X3': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'X3')>,\n",
              " 'Y3': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Y3')>,\n",
              " 'X4': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'X4')>,\n",
              " 'Y4': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'Y4')>}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW_0khtFCZcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37609e79-4cf8-4b65-95ae-4e5f16efd477"
      },
      "source": [
        "string_inputs"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j23p90AT4ewn"
      },
      "source": [
        "def numerical_input_processor(inputs):\n",
        "    if not inputs:\n",
        "        return\n",
        "    \n",
        "    concat = None\n",
        "    if len(inputs.values()) > 1:\n",
        "        concat = tf.keras.layers.Concatenate()(list(inputs.values()))\n",
        "    \n",
        "    norm = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "\n",
        "    for batch, _ in get_dataset(batch_size=DUMMY_BATCH_SIZE).take(1):\n",
        "        data = []\n",
        "        \n",
        "        for k in inputs.keys():\n",
        "            data.append(np.array(batch[k]))    \n",
        "        \n",
        "        data = np.array(data)\n",
        "        data = np.transpose(data)    \n",
        "\n",
        "        norm.adapt(data)\n",
        "\n",
        "    # mean_log = []\n",
        "\n",
        "    for batch, _ in get_dataset(batch_size=BATCH_SIZE):\n",
        "        data = []\n",
        "        \n",
        "        for k in inputs.keys():\n",
        "            data.append(np.array(batch[k]))    \n",
        "        \n",
        "        data = np.array(data)\n",
        "        data = np.transpose(data)\n",
        "\n",
        "        norm.adapt(data)        \n",
        "        # mean_log.append(norm.mean.numpy())    \n",
        "\n",
        "    if concat is not None:\n",
        "        numeric_layer = norm(concat)\n",
        "    else:\n",
        "        numeric_layer = norm(list(inputs.values())[0])\n",
        "    \n",
        "    return numeric_layer"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7idy4ly7I08"
      },
      "source": [
        "def string_input_processor(inputs):\n",
        "    if not inputs:\n",
        "        return\n",
        "\n",
        "    vocabularies = defaultdict(set)\n",
        "\n",
        "    for batch, _ in get_dataset(batch_size=BATCH_SIZE):\n",
        "        for col_name in inputs.keys():        \n",
        "            for st in np.array(batch[col_name]).astype(\"str\"):\n",
        "                vocabularies[col_name].add(st.lower().strip())\n",
        "    \n",
        "    processed_string_inputs = []\n",
        "\n",
        "    for col_name, col_input in inputs.items():    \n",
        "        lookup = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=list(vocabularies[col_name]))\n",
        "        one_hot = tf.keras.layers.experimental.preprocessing.CategoryEncoding(max_tokens = lookup.vocab_size())\n",
        "        \n",
        "        x = tf.strings.lower(col_input)\n",
        "        x = tf.strings.strip(x)\n",
        "        x = lookup(x)\n",
        "        x = one_hot(x)\n",
        "        \n",
        "        processed_string_inputs.append(x)\n",
        "    \n",
        "    if len(processed_string_inputs) > 1:\n",
        "        concat = tf.keras.layers.Concatenate()(processed_string_inputs)\n",
        "        return concat    \n",
        "    else:\n",
        "        return processed_string_inputs[0]    "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge1EBnEH68K9"
      },
      "source": [
        "## PreProcess the inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF1oFqG36zSQ"
      },
      "source": [
        "integer_layer = numerical_input_processor(integer_inputs)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1txErt4UN1qO"
      },
      "source": [
        "integer_layer"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVtNHVBp7AA3"
      },
      "source": [
        "float_layer = numerical_input_processor(float_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9ovbxzKN3jK"
      },
      "source": [
        "float_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pui1mu09EFk"
      },
      "source": [
        "string_layer = string_input_processor(string_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjwMnL4sN443"
      },
      "source": [
        "string_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2oFptl_7A3h"
      },
      "source": [
        "## add all the input layers to a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwTX7EJXCZdA"
      },
      "source": [
        "preprocessed_inputs = []\n",
        "\n",
        "if integer_layer is not None:\n",
        "    preprocessed_inputs.append(integer_layer)\n",
        "if float_layer is not None:\n",
        "    preprocessed_inputs.append(float_layer)\n",
        "if string_layer is not None:\n",
        "    preprocessed_inputs.append(string_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocFKRqBONu--"
      },
      "source": [
        "preprocessed_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXwXXzK07HGV"
      },
      "source": [
        "## Concatenate all the inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfLNTcf7CZdg"
      },
      "source": [
        "if len(preprocessed_inputs) > 1:\n",
        "    preprocessed_inputs_cat = tf.keras.layers.Concatenate()(preprocessed_inputs)\n",
        "else:\n",
        "    preprocessed_inputs_cat = preprocessed_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft7Gxcxe7Pd-"
      },
      "source": [
        "## Create a preprocessing keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWljb4DECZdl"
      },
      "source": [
        "preprocessing_head = tf.keras.Model(model_inputs, preprocessed_inputs_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-qEvPxbCZdp"
      },
      "source": [
        "tf.keras.utils.plot_model(model = preprocessing_head, rankdir=\"LR\", dpi=72, show_shapes=True, expand_nested=True, to_file=\"preprocessing_head.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQQ6SIWq7Z2V"
      },
      "source": [
        "## You can also see the preprocessed outputs for the given inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHO9IDTlCZdu"
      },
      "source": [
        "# preprocessing_head({\n",
        "#     \"sex\" : np.array([\"male\"]),\n",
        "#     \"age\" : np.array([22.0]),\n",
        "#     \"n_siblings_spouses\" : np.array([1]),\n",
        "#     \"parch\" : np.array([0]),\n",
        "#     \"fare\" : np.array([7.25]),\n",
        "#     \"class\" : np.array([\"Third\"]),\n",
        "#     \"deck\" : np.array([\"unknown\"]),\n",
        "#     \"embark_town\" : np.array([\"Southampton\"]),\n",
        "#     \"alone\" : np.array([\"n\"]),\n",
        "# })\n",
        "\n",
        "# preprocessing_head({\n",
        "#     \"age\" : np.array([22.0]),\n",
        "#     \"fare\" : np.array([7.25]),\n",
        "# })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrG1kKdQ7uvY"
      },
      "source": [
        "## pass the model_inputs through the preprocessing_head"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8kjuAizCZd0"
      },
      "source": [
        "preprocessed_outputs = preprocessing_head(model_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_tSQp_oWOxD"
      },
      "source": [
        "num_preprocessed_outputs = list(preprocessed_outputs.shape)[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLkpTLhkWwA-"
      },
      "source": [
        "num_preprocessed_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSIFV_Z379qd"
      },
      "source": [
        "## Find out how many batches should be used for training and how many for validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMqp3rc1CZeL"
      },
      "source": [
        "dataset_size = 0\n",
        "\n",
        "for _ in get_dataset(batch_size=BATCH_SIZE):\n",
        "    dataset_size += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbiyFVmeCZeP"
      },
      "source": [
        "train_size = int(TRAIN_FRAC * dataset_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL6na72JCZeT"
      },
      "source": [
        "print(dataset_size)\n",
        "print(train_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llhVqkwX8G9S"
      },
      "source": [
        "## Split the training and validation datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkX0u5hrCZeY"
      },
      "source": [
        "dataset = get_dataset(batch_size=BATCH_SIZE)\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThC1tnh38Q4j"
      },
      "source": [
        "## Set a caching mechanism that prefetches the next batch while the current batch is training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekIo_-DrCZec"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "train_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcAyAgwi8fJP"
      },
      "source": [
        "## define early stopping and checkpointing callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US9kyo4WCZeg"
      },
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(CHECKPOINT_DIR, \"M.{epoch:02d}-{val_loss:.2f}\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv-2T0XNCZej"
      },
      "source": [
        "callbacks = [\n",
        "    early_stopping_callback,\n",
        "    checkpoint_callback,\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X4ZACIeCZeo"
      },
      "source": [
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
        "    def on_train_end(*args, **kwargs):\n",
        "        IPython.display.clear_output(wait = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OwVCPhw8opd"
      },
      "source": [
        "## model builder function is used by keras-tuner to find the best parameters for a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGy3Grk2CZeA"
      },
      "source": [
        "def model_builder(hp):\n",
        "    if TASK == \"r\":\n",
        "        loss_fn = \"mean_absolute_error\"\n",
        "    elif TASK == \"c\":\n",
        "        if OUTPUT_NODES == 1:\n",
        "            loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        else:\n",
        "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    if TASK == \"r\":\n",
        "        metrics = None\n",
        "    elif TASK == \"c\":\n",
        "        metrics = [\"accuracy\"]\n",
        "\n",
        " \n",
        "    kernel_hp = hp.Choice('kernel_regularization', values = [0.01, 0.001, 0.0001, 0.00001])\n",
        "    activation_hp = hp.Choice(\"activation\", values=[\"elu\", \"relu\"])\n",
        "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "        0.01,\n",
        "        decay_steps=train_size*1000,\n",
        "        decay_rate=1,\n",
        "        staircase=False\n",
        "    )\n",
        "\n",
        "\n",
        "    body = tf.keras.Sequential()\n",
        "    current_nodes = num_preprocessed_outputs\n",
        "\n",
        "    while current_nodes > OUTPUT_NODES:\n",
        "        body.add(\n",
        "            tf.keras.layers.Dense(current_nodes,\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(kernel_hp),\n",
        "                                  activation=activation_hp)\n",
        "        )\n",
        "        body.add(tf.keras.layers.Dropout(0.1))\n",
        "\n",
        "        current_nodes = current_nodes // 2\n",
        "\n",
        "    body.add(tf.keras.layers.Dense(OUTPUT_NODES))\n",
        "\n",
        "    result = body(preprocessed_outputs)\n",
        "    \n",
        "    model = tf.keras.Model(model_inputs, result)\n",
        "    \n",
        "    model.compile(loss=loss_fn, \n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n",
        "                  metrics=metrics\n",
        "    )\n",
        "  \n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2o1-WvS8_Mn"
      },
      "source": [
        "## initialize keras tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLobV8LzCZes"
      },
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective = 'val_loss', \n",
        "                     max_epochs = 10,\n",
        "                     factor = 3,\n",
        "                     directory = 'tuner_dir',\n",
        "                     project_name = 'tuner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NukptGVuCZex"
      },
      "source": [
        "tuner.search(\n",
        "    train_dataset, \n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    callbacks = [ClearTrainingOutput()]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6zEHAuTCZe1"
      },
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC-QWd_s9E_P"
      },
      "source": [
        "## These are the best model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SfHTkWjCZe5"
      },
      "source": [
        "best_hps.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKlcMuIjCZe9"
      },
      "source": [
        "model = tuner.hypermodel.build(best_hps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dvmajfwCZfD"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZxzMXvcCZfI"
      },
      "source": [
        "tf.keras.utils.plot_model(model = model, rankdir=\"LR\", dpi=96, show_shapes=True, expand_nested=True, to_file=\"model.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YtdM9SS9Owf"
      },
      "source": [
        "## Find out training and validation losses with untrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8s4BwAZCZfM"
      },
      "source": [
        "# loss with un-trained model\n",
        "model.evaluate(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8cfLHi4CZfS"
      },
      "source": [
        "# val_loss with un-trained model\n",
        "model.evaluate(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL902YSw9WgM"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uRnEVasXCZfX"
      },
      "source": [
        "history = model.fit(\n",
        "    train_dataset, \n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks,\n",
        "    epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5MEaYxH9a40"
      },
      "source": [
        "## Find out training and validation losses with trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbsZlC7LCZfb"
      },
      "source": [
        "# loss with trained model\n",
        "model.evaluate(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k7dH2YgCZfg"
      },
      "source": [
        "# val_loss with trained model\n",
        "model.evaluate(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH3I7X3VCZfj"
      },
      "source": [
        "def plot_loss(history):\n",
        "    plt.plot(history.history['loss'], label='loss')\n",
        "    plt.plot(history.history['val_loss'], label='val_loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qawO4E_sCZfn"
      },
      "source": [
        "plot_loss(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caSpNHWBCZfq"
      },
      "source": [
        "def plot_acc(history):\n",
        "    plt.plot(history.history['accuracy'], label='accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XgjeDoQCZfv"
      },
      "source": [
        "if TASK == \"c\":\n",
        "    plot_acc(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wAqQthi-RS_"
      },
      "source": [
        "## Test the model on some dummy data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SARRjv3CZf3"
      },
      "source": [
        "# model.predict({\n",
        "#     \"sex\" : np.array([\"male\"]),\n",
        "#     \"age\" : np.array([22.0]),\n",
        "#     \"n_siblings_spouses\" : np.array([1]),\n",
        "#     \"parch\" : np.array([0]),\n",
        "#     \"fare\" : np.array([7.25]),\n",
        "#     \"class\" : np.array([\"Third\"]),\n",
        "#     \"deck\" : np.array([\"unknown\"]),\n",
        "#     \"embark_town\" : np.array([\"Southampton\"]),\n",
        "#     \"alone\" : np.array([\"n\"]),\n",
        "# })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPl-FWY9CZf8"
      },
      "source": [
        "# reloaded = tf.keras.models.load_model(\"models/titanic_model.24-1610.52\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIMEv9r_CZgB"
      },
      "source": [
        "# reloaded.evaluate(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A75qm1uPCZgG"
      },
      "source": [
        "# reloaded.evaluate(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}